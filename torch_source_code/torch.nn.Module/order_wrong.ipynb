{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module基类常用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> add_module(name, module)\n",
    "\n",
    "将子模块添加到当前模块。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> register_module()\n",
    "\n",
    "add_module方法的封装，用于将新的`name:module`键值对加入module中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> apply(fn)\n",
    "\n",
    "+ 递归地将函数应用于所有子模块。\n",
    "+ apply方法可以用于任何submodule（通过.children()或者self.获取到的）\n",
    "+ 常用来初始化模型参数（同torch.nn.init）\n",
    "\n",
    "example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "ReLU()\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "ReLU()\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "@torch.no_grad()  # 不计算梯度，不反向传播\n",
    "def init_weights(m):\n",
    "    print(m)\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.fill_(1.0)\n",
    "        print(m.weight)\n",
    "net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2),nn.ReLU(),nn.Sequential(nn.Linear(2, 2),nn.ReLU()))\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> buffers(recurse=True)\n",
    "\n",
    "模型中需要保存下来的参数包括两种：\n",
    "\n",
    "+ 一种是反向传播需要更新：parameter，可以通过parameter()返回\n",
    "+ 一种是反向传播不需要更新的：buffer，可以通过buffer()返回\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> children()：\n",
    "返回网络模型里的组成元素的迭代器。类似`modules()`方法。二者对比可参考：[https://blog.csdn.net/u013066730/article/details/94600978](https://blog.csdn.net/u013066730/article/details/94600978)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> register_buffer(name: str, tensor: Optional[Tensor], persistent: bool = True)\n",
    "\n",
    "在当前模块中添加一个buffer变量，例如，现在需要手写一个BatchNorm，那么其`running_mean`并不是一个parameter，这就需要用下述方式注册一个buffer：\n",
    "\n",
    "```python\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self,..):\n",
    "        self.register_buffer('running_mean',torch.zeros(num_features))\n",
    "        self.register_buffer('running_variance',torch.ones(num_features))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> register_parameter(self, name: str, param: Optional[Parameter])\n",
    "\n",
    "用于在当前模块中添加一个parameter变量，其中参数param是一个Parameter类型（继承至tensor类型，nn.parameter.Parameter）。\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GaussianModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GaussianModel, self).__init__()\n",
    "\n",
    "        self.register_parameter('mean', nn.Parameter(torch.zeros(1),\n",
    "                                                     requires_grad=True))\n",
    "        \n",
    "        self.pdf = torch.distributions.Normal(self.state_dict()['mean'],\n",
    "                                              torch.tensor([1.0]))\n",
    "    def forward(self, x):\n",
    "        return -self.pdf.log_prob(x)\n",
    "\n",
    "model = GaussianModel()\n",
    "for name, param in model.named_parameters():\n",
    "    print(name,param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "ReLU()\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for i in net.children():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> cpu() / cuda()\n",
    "\n",
    "将模型的parameters和buffers移动到CPU/GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> eval()\n",
    "\n",
    "将module设置为验证模式，会影响一些特定modules，如：Dropout，BatchNorm等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> load_state_dict(state_dict, strict=True)\n",
    "\n",
    "将 state_dict 中的参数(parameters)和缓冲区(buffers)复制到此模块及其子模块中。如果 strict 为 True，则 state_dict 的键必须与该模块的 state_dict() 函数返回的键完全匹配。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> named_buffers(prefix='', recurse=True)\n",
    "\n",
    "返回module buffers' name的迭代器，example： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, buf in net.named_buffers():\n",
    "   if name in ['running_var']:\n",
    "       print(buf.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> named_children()\n",
    "\n",
    "返回直接子模块的迭代器，产生模块的名称以及模块本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Linear(in_features=2, out_features=2, bias=True)\n",
      "1 Linear(in_features=2, out_features=2, bias=True)\n",
      "2 ReLU()\n",
      "3 Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for name, module in net.named_children():\n",
    "    # if name in ['conv4', 'conv5']:\n",
    "    print(name,module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> named_modules\n",
    "\n",
    "返回网络中所有模块的迭代器，产生模块的名称以及模块本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "0 Linear(in_features=2, out_features=2, bias=True)\n",
      "1 Linear(in_features=2, out_features=2, bias=True)\n",
      "2 ReLU()\n",
      "3 Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "3.0 Linear(in_features=2, out_features=2, bias=True)\n",
      "3.1 ReLU()\n"
     ]
    }
   ],
   "source": [
    "for name, module in net.named_modules():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> named_parameters(prefix='', recurse=True)\n",
    "\n",
    "返回模块参数的迭代器，产生参数的名称以及参数本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([2, 2])\n",
      "0.bias torch.Size([2])\n",
      "1.weight torch.Size([2, 2])\n",
      "1.bias torch.Size([2])\n",
      "3.0.weight torch.Size([2, 2])\n",
      "3.0.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name,param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> get_submodule(target: str) -> 'Module'\n",
    "\n",
    "从Module中获取子module，example："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=2, bias=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_submodule('3.0')  # 获取第0个子模块的内嵌滴0个子模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> state_dict()\n",
    "\n",
    "返回包含模块整个状态的字典。 包括参数和持久缓冲区（例如运行平均值）。键是对应的参数和缓冲区名称。不包括设置为 None 的参数和缓冲区。常用于保存模型参数。\n",
    "\n",
    "保存模型例子：\n",
    "\n",
    "```python\n",
    "# Additional information\n",
    "EPOCH = 5\n",
    "PATH = \"model.pt\"\n",
    "LOSS = 0.4\n",
    "\n",
    "torch.save({\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': LOSS,\n",
    "            }, PATH)\n",
    "```\n",
    "\n",
    "加载模型例子：\n",
    "```python\n",
    "model = Net()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "# - or -\n",
    "model.train()\n",
    "```\n",
    "更多详情参考：[https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> get_parameter(target: str)\n",
    "\n",
    "根据参数名得到参数，exp："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_parameter('1.weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> get_buffer(target: str)\n",
    "\n",
    "根据buffer名得到buffer值，用法同get_parameter。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _apply(fn)\n",
    "\n",
    "+ 对所有的module、parameter、buffer都进行一个fn\n",
    "\n",
    "Example：.cpu / .cuda()源码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Module:\n",
    "    def cuda(self: T, device: Optional[Union[int, device]] = None) -> T:\n",
    "        r\"\"\"Moves all model parameters and buffers to the GPU.\n",
    "\n",
    "        This also makes associated parameters and buffers different objects. So\n",
    "        it should be called before constructing optimizer if the module will\n",
    "        live on GPU while being optimized.\n",
    "\n",
    "        .. note::\n",
    "            This method modifies the module in-place.\n",
    "\n",
    "        Args:\n",
    "            device (int, optional): if specified, all parameters will be\n",
    "                copied to that device\n",
    "\n",
    "        Returns:\n",
    "            Module: self\n",
    "        \"\"\"\n",
    "        return self._apply(lambda t: t.cuda(device))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> type(dst_type)\n",
    "\n",
    "将所有的parameters和buffers转化为目标数据类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "————————————————————————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据格式函数\n",
    "\n",
    "> float() / double() / half() / bfloat16() \n",
    "\n",
    "将所有的parameters和buffers转化为指定的数据类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> to_empty()\n",
    "\n",
    "把模型parameter和buffers移动到指定device上（不保存其具体数值）。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torchcpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa491c18d0be0c4e8a787577469a16b94dd30119a7d01c99fcb9d2f4862fba4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
