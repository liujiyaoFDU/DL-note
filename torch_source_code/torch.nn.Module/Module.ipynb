{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch | 深入剖析torch.nn.Module方法及源码\n",
    "\n",
    "torch.nn是一个专门为神经网络设计的模块化接口，包含卷积、池化、线性等计算，以及其他如loss等，可以将torch.nn中的每一个模块看做神经网络中的每一层。\n",
    "\n",
    "torch.nn.Module是网络模型的一个基类，大部分自定义的子模型（卷积、池化甚至整个网络）是这个基类的子类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、`class torch.nn.Parameter`\n",
    "\n",
    "功能：`torch.nn.Parameter`是继承至`torch.tensor`的子类，Parameter类型会自动被认为是module的可训练参数，即加入`.parameter()`迭代器中。\n",
    "\n",
    "exp: `torch.nn.Parameter(torch.tensor[3.14159],requore_grad=True)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、`class torch.nn.Module`\n",
    "\n",
    "### 2.1 构建模型\n",
    "\n",
    "首先我们看看如何定义一个Module："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module类包含48个方法，下面我们来看一下这些方法如何使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 子模型操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> register_module()\n",
    "\n",
    "add_module方法的封装，用于将新的`name:module`键值对加入module中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> add_module(name, module)\n",
    "\n",
    "将子模块添加到当前模块。example："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # 下面是两种等价方式\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.add_module(\"conv2\", nn.Conv2d(1, 20, 5))\n",
    "\n",
    "model = Model()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 访问子模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> children()：\n",
    "\n",
    "返回网络模型里的组成元素的迭代器。类似`modules()`方法。二者对比可参考：[https://blog.csdn.net/u013066730/article/details/94600978](https://blog.csdn.net/u013066730/article/details/94600978)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "for i in model.children():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> named_children()\n",
    "\n",
    "返回直接子模块的迭代器，产生模块的名称以及模块本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "conv2 Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_children():\n",
    "    print(name,module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `modules()`\n",
    "\n",
    "返回当前模型所有模型的迭代器，重复的模型只被返回一次。与`children()`方法不同，`modules()`方法还会返回当前模型module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "ReLU()\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "ReLU()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2, 2), \n",
    "                    nn.Linear(2, 2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Sequential(nn.Linear(2, 2),\n",
    "                                    nn.ReLU())\n",
    "                    )\n",
    "for module in net.modules():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> named_modules\n",
    "\n",
    "返回网络中所有模块的迭代器，产生模块的名称以及模块本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GaussianModel()\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> get_submodule(target: str) -> 'Module'\n",
    "\n",
    "从Module中获取子module，example："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 模型参数（parameter）与缓冲区（buffer）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> register_parameter(self, name: str, param: Optional[Parameter])\n",
    "\n",
    "用于在当前模块中添加一个parameter变量，其中参数param是一个Parameter类型（继承至tensor类型，nn.parameter.Parameter）。\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GaussianModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GaussianModel, self).__init__()\n",
    "\n",
    "        self.register_parameter('mean', nn.Parameter(torch.zeros(1),\n",
    "                                                     requires_grad=True))\n",
    "        \n",
    "        self.pdf = torch.distributions.Normal(self.state_dict()['mean'],\n",
    "                                              torch.tensor([1.0]))\n",
    "    def forward(self, x):\n",
    "        return -self.pdf.log_prob(x)\n",
    "\n",
    "model = GaussianModel()\n",
    "for name, param in model.named_parameters():\n",
    "    print(name,param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> parameters(recurse=True)\n",
    "\n",
    "返回模型参数的迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(type(param), param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> named_parameters(prefix='', recurse=True)\n",
    "\n",
    "返回模块参数的迭代器，产生参数的名称以及参数本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([2, 2])\n",
      "0.bias torch.Size([2])\n",
      "1.weight torch.Size([2, 2])\n",
      "1.bias torch.Size([2])\n",
      "3.0.weight torch.Size([2, 2])\n",
      "3.0.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name,param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> get_parameter(target: str)\n",
    "\n",
    "根据参数名得到参数，exp："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1466, -0.1264],\n",
       "        [ 0.2812,  0.1436]], requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_parameter('1.weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> buffers(recurse=True)\n",
    "\n",
    "模型中需要保存下来的参数包括两种：\n",
    "\n",
    "+ 一种是反向传播需要更新：parameter，可以通过parameter()返回\n",
    "+ 一种是反向传播不需要更新的：buffer，可以通过buffer()返回"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> named_buffers(prefix='', recurse=True)\n",
    "\n",
    "返回module buffers' name的迭代器，example： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, buf in net.named_buffers():\n",
    "    print(buf.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> register_buffer(name: str, tensor: Optional[Tensor], persistent: bool = True)\n",
    "\n",
    "在当前模块中添加一个buffer变量，例如，现在需要手写一个BatchNorm，那么其`running_mean`并不是一个parameter，这就需要用下述方式注册一个buffer：\n",
    "\n",
    "```python\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self,..):\n",
    "        self.register_buffer('running_mean',torch.zeros(num_features))\n",
    "        self.register_buffer('running_variance',torch.ones(num_features))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> get_buffer(target: str)\n",
    "\n",
    "根据buffer名得到buffer值，用法同get_parameter。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 数据格式及转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> float() / double() / half() / bfloat16() \n",
    "\n",
    "将所有的parameters和buffers转化为指定的数据类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> type(dst_type)\n",
    "\n",
    "将所有的parameters和buffers转化为目标数据类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 模型移动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> to_empty()\n",
    "\n",
    "把模型parameter和buffers移动到指定device上（不保存其具体数值）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> cpu() / cuda() / xpu(）\n",
    "\n",
    "将模型的parameters和buffers移动到CPU/GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 模型模式调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> train(mode=True)\n",
    "将该模块设置为train训练模式。默认值：True。\n",
    "\n",
    "> eval()\n",
    "\n",
    "将module设置为验证模式，会影响一些特定modules，如：Dropout，BatchNorm等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 其他\n",
    "\n",
    "> zero_grad(set_to_none=False)\n",
    "\n",
    "将所有模型参数的梯度设置为零。set_to_none=True会让内存分配器来处理梯度，而不是主动将它们设置为0，这样会适度加速。\n",
    "\n",
    "> forward(*input)\n",
    "\n",
    "方法定义了神经网络每次调用时都需要执行的前向传播计算，所有的子类都必须要重写这个方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> apply(fn)\n",
    "\n",
    "+ 递归地将函数应用于所有子模块。\n",
    "+ apply方法可以用于任何submodule（通过.children()或者self.获取到的）\n",
    "+ 常用来初始化模型参数（同torch.nn.init）\n",
    "\n",
    "example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "ReLU()\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "ReLU()\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): ReLU()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "@torch.no_grad()  # 不计算梯度，不反向传播\n",
    "def init_weights(m):\n",
    "    print(m)\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.fill_(1.0)\n",
    "        print(m.weight)\n",
    "net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2),nn.ReLU(),nn.Sequential(nn.Linear(2, 2),nn.ReLU()))\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> load_state_dict(state_dict, strict=True)\n",
    "\n",
    "将 state_dict 中的参数(parameters)和缓冲区(buffers)复制到此模块及其子模块中。如果 strict 为 True，则 state_dict 的键必须与该模块的 state_dict() 函数返回的键完全匹配。\n",
    "\n",
    "> state_dict()\n",
    "\n",
    "返回包含模块整个状态的字典。 包括参数和持久缓冲区（例如运行平均值）。键是对应的参数和缓冲区名称。不包括设置为 None 的参数和缓冲区。常用于保存模型参数。\n",
    "\n",
    "保存模型例子：\n",
    "\n",
    "```python\n",
    "# Additional information\n",
    "EPOCH = 5\n",
    "PATH = \"model.pt\"\n",
    "LOSS = 0.4\n",
    "\n",
    "torch.save({\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': LOSS,\n",
    "            }, PATH)\n",
    "```\n",
    "\n",
    "加载模型例子：\n",
    "```python\n",
    "model = Net()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "# - or -\n",
    "model.train()\n",
    "```\n",
    "更多详情参考：[https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _apply(fn)\n",
    "\n",
    "+ 对所有的module、parameter、buffer都进行一个fn\n",
    "\n",
    "Example：.cpu / .cuda()源码\n",
    "\n",
    "```python\n",
    "class Module:\n",
    "    def cuda(self: T, device: Optional[Union[int, device]] = None) -> T:\n",
    "        r\"\"\"Moves all model parameters and buffers to the GPU.\n",
    "\n",
    "        This also makes associated parameters and buffers different objects. So\n",
    "        it should be called before constructing optimizer if the module will\n",
    "        live on GPU while being optimized.\n",
    "\n",
    "        .. note::\n",
    "            This method modifies the module in-place.\n",
    "\n",
    "        Args:\n",
    "            device (int, optional): if specified, all parameters will be\n",
    "                copied to that device\n",
    "\n",
    "        Returns:\n",
    "            Module: self\n",
    "        \"\"\"\n",
    "        return self._apply(lambda t: t.cuda(device))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、hook方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了节省内存，pytorch在计算过程中不保存中间变量，包括中间层的特征图和非叶子张量的梯度。为了访问网络的中间变量，我们需要注册`hook`来导出中间变量。利用它，我们可以不必改变网络输入输出的结构，方便地获取、改变网络中间层变量的值和梯度。\n",
    "\n",
    "`hook`方法有四种：\n",
    "\n",
    "+ torch.Tensor.register_hook()\n",
    "+ torch.nn.Module.register_forward_hook()\n",
    "+ torch.nn.Module.register_backward_hook()\n",
    "+ torch.nn.Module.register_forward_pre_hook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 torch.Tensor.register_hook(hook_fn)\n",
    "\n",
    "注册一个反向传播hook函数hook_fn，针对tensor的register_hook函数接收一个输入参数hook_fn，为自定义函数名称。在每次调用backward函数之前都会先调用hook_fn函数。hook_fn函数同样接收一个输入参数，为torch.Tensor张量的梯度。\n",
    "\n",
    "例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient: tensor([55.]) tensor([39.]) None None None\n",
      "hook函数保留中间变量a的梯度值: tensor([15.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home3/HWGroup/liujy/.local/lib/python3.6/site-packages/torch/_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:417.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# x,y 为leaf节点，也就是说，在计算的时候，PyTorch只会保留此节点的梯度值\n",
    "x = torch.tensor([3.], requires_grad=True)\n",
    "y = torch.tensor([5.], requires_grad=True)\n",
    "\n",
    "# a,b,c 均为中间变量，在计算梯度时，此部分会被释放掉\n",
    "a = x + y\n",
    "b = x * y\n",
    "c = a * b\n",
    "# 新建列表，用于存储hook函数保存的中间梯度值\n",
    "a_grad = []\n",
    "def hook_grad(grad):\n",
    "    a_grad.append(grad)\n",
    "\n",
    "# register_hook的参数为一个函数\n",
    "handle = a.register_hook(hook_grad)\n",
    "c.backward()\n",
    "\n",
    "# 只有leaf节点才会有梯度值\n",
    "print('gradient:', x.grad, y.grad, a.grad, b.grad, c.grad)\n",
    "# hook函数保留中间变量a的梯度值\n",
    "print('hook函数保留中间变量a的梯度值:', a_grad[0])\n",
    "# 移除hook函数\n",
    "handle.remove()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 torch.nn.Module.register_forward_hook(hook_fn)\n",
    "\n",
    "用法：在神经网络模型module上注册一个forward_hook函数hook_fn，register_forward_hook函数接收一个输入参数hook_fn，为自定义函数名称。注：在调用hook_fn函数的那个模型（层）进行前向传播并计算得到结果之后才会执行hook_fn函数，因此修改output值仅会对后续操作产生影响。hook_fn函数接收三个输入参数：module，input，output，其中module为当前网络层，input为当前网络层输入数据，output为当前网络层输出数据。例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def print_shape(model, input, output):\n",
    "    print(model)\n",
    "    print(input[0].shape, '=>', output.shape)\n",
    "    print('====================================')\n",
    "\n",
    "\n",
    "def get_children(model: nn.Module):\n",
    "    # get children from model(取出所有子model及嵌套model)\n",
    "    children = list(model.children())\n",
    "    flatt_children = []\n",
    "    if children == []:\n",
    "        return model\n",
    "    else:\n",
    "        for child in children:\n",
    "            try: \n",
    "                flatt_children.extend(get_children(child))\n",
    "            except TypeError:\n",
    "                flatt_children.append(get_children(child))\n",
    "    return flatt_children\n",
    "\n",
    "model_name = 'vgg11'\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "flatt_children = get_children(model)\n",
    "for layer in flatt_children:\n",
    "    layer.register_forward_hook(print_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([4, 3, 299, 299]) => torch.Size([4, 64, 299, 299])\n",
      "====================================\n",
      "ReLU(inplace=True)\n",
      "torch.Size([4, 64, 299, 299]) => torch.Size([4, 64, 299, 299])\n",
      "====================================\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "torch.Size([4, 64, 299, 299]) => torch.Size([4, 64, 149, 149])\n",
      "====================================\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([4, 64, 149, 149]) => torch.Size([4, 128, 149, 149])\n",
      "====================================\n",
      "ReLU(inplace=True)\n",
      "torch.Size([4, 128, 149, 149]) => torch.Size([4, 128, 149, 149])\n",
      "====================================\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "torch.Size([4, 128, 149, 149]) => torch.Size([4, 128, 74, 74])\n",
      "====================================\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([4, 128, 74, 74]) => torch.Size([4, 256, 74, 74])\n",
      "====================================\n",
      "ReLU(inplace=True)\n",
      "torch.Size([4, 256, 74, 74]) => torch.Size([4, 256, 74, 74])\n",
      "====================================\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([4, 256, 74, 74]) => torch.Size([4, 256, 74, 74])\n",
      "====================================\n",
      "ReLU(inplace=True)\n",
      "torch.Size([4, 256, 74, 74]) => torch.Size([4, 256, 74, 74])\n",
      "====================================\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "torch.Size([4, 256, 74, 74]) => torch.Size([4, 256, 37, 37])\n",
      "====================================\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([4, 256, 37, 37]) => torch.Size([4, 512, 37, 37])\n",
      "====================================\n",
      "ReLU(inplace=True)\n",
      "torch.Size([4, 512, 37, 37]) => torch.Size([4, 512, 37, 37])\n",
      "====================================\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([4, 512, 37, 37]) => torch.Size([4, 512, 37, 37])\n",
      "====================================\n",
      "ReLU(inplace=True)\n",
      "torch.Size([4, 512, 37, 37]) => torch.Size([4, 512, 37, 37])\n",
      "====================================\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "torch.Size([4, 512, 37, 37]) => torch.Size([4, 512, 18, 18])\n",
      "====================================\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([4, 512, 18, 18]) => torch.Size([4, 512, 18, 18])\n",
      "====================================\n",
      "ReLU(inplace=True)\n",
      "torch.Size([4, 512, 18, 18]) => torch.Size([4, 512, 18, 18])\n",
      "====================================\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([4, 512, 18, 18]) => torch.Size([4, 512, 18, 18])\n",
      "====================================\n",
      "ReLU(inplace=True)\n",
      "torch.Size([4, 512, 18, 18]) => torch.Size([4, 512, 18, 18])\n",
      "====================================\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "torch.Size([4, 512, 18, 18]) => torch.Size([4, 512, 9, 9])\n",
      "====================================\n",
      "Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1))\n",
      "torch.Size([4, 512, 9, 9]) => torch.Size([4, 4096, 3, 3])\n",
      "====================================\n",
      "ReLU(inplace=True)\n",
      "torch.Size([4, 4096, 3, 3]) => torch.Size([4, 4096, 3, 3])\n",
      "====================================\n",
      "Dropout(p=0.0, inplace=False)\n",
      "torch.Size([4, 4096, 3, 3]) => torch.Size([4, 4096, 3, 3])\n",
      "====================================\n",
      "Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1))\n",
      "torch.Size([4, 4096, 3, 3]) => torch.Size([4, 4096, 3, 3])\n",
      "====================================\n",
      "ReLU(inplace=True)\n",
      "torch.Size([4, 4096, 3, 3]) => torch.Size([4, 4096, 3, 3])\n",
      "====================================\n",
      "AdaptiveAvgPool2d(output_size=1)\n",
      "torch.Size([4, 4096, 3, 3]) => torch.Size([4, 4096, 1, 1])\n",
      "====================================\n",
      "Linear(in_features=4096, out_features=1000, bias=True)\n",
      "torch.Size([4, 4096]) => torch.Size([4, 1000])\n",
      "====================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3597, -0.1723, -0.2330,  ..., -1.0403,  0.1759,  2.3089],\n",
       "        [-1.4455, -0.0422, -0.2029,  ..., -0.8731,  0.0511,  2.3164],\n",
       "        [-1.3242, -0.0774, -0.1487,  ..., -1.0805,  0.0708,  2.2107],\n",
       "        [-1.3521, -0.2369, -0.2137,  ..., -0.9622, -0.0346,  2.2769]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input = torch.randn(4,3,299,299)\n",
    "model(batch_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 torch.nn.Module.register_forward_pre_hook(hook_fn)\n",
    "\n",
    "功能：用来导出或修改指定子模型的输入张量，需要使用return返回修改后的output值使操作生效。\n",
    "\n",
    "用法：在神经网络模型module上注册一个forward_pre_hook函数hook_fn，register_forward_pre_hook函数接收一个输入参数hook_fn，为自定义函数名称。注：在调用hook_fn函数的那个模型（层）进行前向传播操作之前会先执行hook_fn函数，因此修改input值会对该层的操作产生影响，该层的运算结果被继续向前传递。hook_fn函数接收两个输入参数：module，input，其中module为当前网络层，input为当前网络层输入数据。下面代码执行的功能是 3 × 3 3 \\times 33×3 的卷积和 2 × 2 2 \\times 22×2 的池化。我们使用register_forward_pre_hook函数修改中间卷积层输入的张量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------执行forward函数-------------\n",
      "卷积层输入： tensor([[[[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]]]])\n",
      "-------------执行forward_pre_hook函数-------------\n",
      "-------------结束forward_pre_hook函数-------------\n",
      "卷积层输出： tensor([[[[ 9.,  9.],\n",
      "          [ 9.,  9.]],\n",
      "\n",
      "         [[18., 18.],\n",
      "          [18., 18.]]]], grad_fn=<SlowConv2DBackward0>)\n",
      "池化层输出： tensor([[[[ 9.]],\n",
      "\n",
      "         [[18.]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "-------------结束forward函数-------------\n",
      "神经网络模型输出：\n",
      "output shape: torch.Size([1, 2, 1, 1])\n",
      "output value: tensor([[[[ 9.]],\n",
      "\n",
      "         [[18.]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 2, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"-------------执行forward函数-------------\")\n",
    "        print(\"卷积层输入：\",x)\n",
    "        x = self.conv1(x)\n",
    "        print(\"卷积层输出：\",x)\n",
    "        x = self.pool1(x)\n",
    "        print(\"池化层输出：\",x)\n",
    "        print(\"-------------结束forward函数-------------\")\n",
    "        return x\n",
    "\n",
    "# module为net.conv1\n",
    "# data_input为net.conv1层输入\n",
    "def forward_pre_hook(module, data_input):\n",
    "    print(\"-------------执行forward_pre_hook函数-------------\")\n",
    "    input_block.append(data_input)\n",
    "    #print(\"修改前的卷积层输入：{}\".format(data_input))\n",
    "    #data_input = torch.rand((1, 1, 4, 4))\n",
    "    #print(\"修改后的卷积层输入：{}\".format(data_input))\n",
    "    print(\"-------------结束forward_pre_hook函数-------------\")\n",
    "    #return data_input\n",
    "\n",
    "# 初始化网络\n",
    "net = Net()\n",
    "net.conv1.weight[0].detach().fill_(1)\n",
    "net.conv1.weight[1].detach().fill_(2)\n",
    "net.conv1.bias.data.detach().zero_()\n",
    "\n",
    "# 注册hook\n",
    "input_block = list()\n",
    "handle = net.conv1.register_forward_pre_hook(forward_pre_hook)\n",
    "\n",
    "# inference\n",
    "fake_img = torch.ones((1, 1, 4, 4))  # batch size * channel * H * W\n",
    "output = net(fake_img)\n",
    "handle.remove()\n",
    "\n",
    "# 观察\n",
    "print(\"神经网络模型输出：\\noutput shape: {}\\noutput value: {}\\n\".format(output.shape, output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 torch.nn.Module.register_backward_hook(hook)\n",
    "\n",
    "网络在进行反向传播时，可以通过register_backward_hook来获取中间层的梯度输入和输出，常用来实现特征图梯度的提取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('yolov5': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc1f57a4b107e657fcf6f6e5f61550706ff2a6a2746ba32ccd89b21e7725f021"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
